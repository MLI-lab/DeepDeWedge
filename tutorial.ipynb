{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDeWedge Tutorial\n",
    "\n",
    "This is a minimal example for how to apply DeepDeWedge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from dataset import setup_fitting_and_val_dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from utils.mrctools import load_mrc_data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from model import Unet3D\n",
    "from utils.fitting import masked_loss, get_avg_model_input_mean_and_var\n",
    "from utils.mrctools import save_mrc_data\n",
    "from utils.visualization import plot_tomo_slices\n",
    "import datetime\n",
    "import shutil\n",
    "from utils.missing_wedge import fft_3d\n",
    "from utils.dataloader import MultiEpochsDataLoader\n",
    "from torchsummary import summary\n",
    "from refine_tomogram import refine_tomogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the tutorial dataset\n",
    "We apply DeepDeWedge to the Wiener-Filter CTF corrected FBP reconstructon of tilt series 05 of EMPIAR-10045. First, we download the tutorial dataset which contains FBP reconstructions from the even, odd and full tilt series. We binned these reconstructions by a factor of 6 using average pooling, which results in a physical voxel size of 13.02 Angstroms. For CTF correction, we used the Wiener-like filter implemented in IsoNet (https://github.com/IsoNet-cryoET/IsoNet).\n",
    "\n",
    "The following two lines of code download the data as .zip archive and unzip it. Upon successful completion, you will find a new subdirectory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data with wget\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_yuI2Xu2ISnuBKT3FS_9cqdAXvC2Shh8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1_yuI2Xu2ISnuBKT3FS_9cqdAXvC2Shh8\" -O tutorial_data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip tutorial_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"tutorial_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomo_full = load_mrc_data(\"./tutorial_data/IS002_291013_006_FullFBP_Bin6_Wiener.mrc\")\n",
    "plot_tomo_slices(tomo_full, figsize=(20, 10))\n",
    "print(tomo_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup datasets for model fitting\n",
    "We first load the dataset for model fitting. The function `setup_fitting_and_val_dataset` returns two torch datasets, one for model fitting and one for validation to prevent overfitting. The tomograms corresponding to the filepaths in `tomo0_files` are used to construct the model inputs, while the ones in `tomo1_files` are used to construct the targets. \n",
    "\n",
    "Both the fitting and the validation dataset return model inputs and targets with shape `subtomo_size x subtomo_size x subtomo_size`. These subtomograms are extracted from the tomograms using x, y and z direction strides specified in `extraction_strides`. To reduce RAM consumption during model fitting, all subtomograms are saved to the directory `save_subtomos_to`, which is created if it doe note exist.\n",
    "\n",
    "The number of elements in the validation set is at most `validation_frac` times the number of total extracted subtomograms. It may also contain fewer subtomograms since we randomly sample the validation subtomograms such that they have no overlap with the ones used for model fitting. If the sampling procedure was unable to sample enough validation subtomograms, the function prints a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./subtomos/\", ignore_errors=True)\n",
    "\n",
    "fitting_dataset, val_dataset = setup_fitting_and_val_dataset(\n",
    "    tomo0_files=[\"./tutorial_data/IS002_291013_006_EvenFBP_Bin6_Wiener.mrc\"],\n",
    "    tomo1_files=[\"./tutorial_data/IS002_291013_006_OddFBP_Bin6_Wiener.mrc\"],\n",
    "    subtomo_size=80,\n",
    "    extraction_strides=[40, 40, 40],\n",
    "    mw_angle=60,\n",
    "    val_fraction=0.2,\n",
    "    save_subtomos_to=\"./subtomos/\",\n",
    ")\n",
    "\n",
    "print(f\"Number of subtomograms for model fitting: {len(fitting_dataset)}\")\n",
    "print(f\"Number of subtomos for validation: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the fitting and the validation dataset return dictionries containing the following items:\n",
    "* `model_input`: A model input $\\tilde{\\mathbf{v}}_\\varphi^0$ with two missing wedges\n",
    "* `model_target`: A model target $\\tilde{\\mathbf{v}}_\\varphi^1$ with only one missing wedge\n",
    "* `rot_mw_mask`: The rotated missing wedge mask $\\mathbf{M}_\\varphi$\n",
    "* `mw_mask`: The original missing wedge mask $\\mathbf{M}$. This mask is the same for all elements in the dataset.\n",
    "\n",
    "**Note**: The rotation angles $\\varphi$ in the training set are always random, and are re-sampled every time an item is queried. For the validation dataset, we only sample random rotation angles once and every item always has its fixed rotation.\n",
    "\n",
    "\n",
    "Let's now have a look at the real and Fourier domain representation of some of the model inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    item = fitting_dataset[k]\n",
    "    model_input = item[\"model_input\"]\n",
    "    model_input -= model_input.mean()  \n",
    "    plot_tomo_slices(item[\"model_input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a fitting and a validation dataloader which return batches of elements from the fitting and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_workers = 10\n",
    "\n",
    "fitting_dataloader = torch.utils.data.DataLoader(dataset=fitting_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "First, we setup a 3D U-Net. The architecture we implemented in `unet.py`is the same used in the official IsoNet implementation.\n",
    "Before we can start model fitting, we calculate the average mean and variance of the model inputs in the fitting dataset. We use these values to normalize the input to the U-Net during model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_params = {\n",
    "    \"in_chans\": 1,\n",
    "    \"out_chans\": 1,\n",
    "    \"chans\": 64,\n",
    "    \"num_pool_layers\": 3,\n",
    "    \"drop_prob\": 0.0,\n",
    "}\n",
    "\n",
    "avg_model_input_mean, avg_model_input_var = get_avg_model_input_mean_and_var(\n",
    "    fitting_dataloader, \n",
    "    batches=3*len(fitting_dataloader),\n",
    "    verbose=True\n",
    ")\n",
    "unet_params[\"normalization_loc\"] = avg_model_input_mean\n",
    "unet_params[\"normalization_scale\"] = math.sqrt(avg_model_input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model fitting, we use the PyTorch lightning framework for convenience. Below, we define the class `LitUnet3D`. The class takes parameters for a U-Net and the `torch.optim.Adam` optimizer as input and can then be used to fit the U-Net. The important methods of this class are:\n",
    "* `training_step`: This step handles passing the model inputs provided by the fitting dataloader through the model and calculating the loss. \n",
    "+ `validation_step`: In this step, we can implement any validation routine we like. For simplicity, we just calculate the loss on the validation set to monitor overfitting. Depending on which logger we use, we can also log plots of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUnet3D(pl.LightningModule):\n",
    "    def __init__(self, unet_params, adam_params):\n",
    "        super().__init__()\n",
    "        self.unet_params = unet_params\n",
    "        self.adam_params = adam_params\n",
    "        self.unet = Unet3D(**self.unet_params)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unet(x.unsqueeze(1)).squeeze(1)  # unsqueeze to add channel dimension, squeeze to remove it\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        model_output = self(batch[\"model_input\"])  \n",
    "        loss = masked_loss(\n",
    "            model_output=model_output, \n",
    "            target=batch[\"model_target\"], \n",
    "            rot_mw_mask=batch[\"rot_mw_mask\"], \n",
    "            mw_mask=batch[\"mw_mask\"]\n",
    "        )\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        model_output = self(batch[\"model_input\"])\n",
    "        loss = masked_loss(\n",
    "            model_output=model_output,\n",
    "            target=batch[\"model_target\"], \n",
    "            rot_mw_mask=batch[\"rot_mw_mask\"], \n",
    "            mw_mask=batch[\"mw_mask\"]\n",
    "        )\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **self.adam_params)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = LitUnet3D(unet_params=unet_params, adam_params={\"lr\": 4e-4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightning's ``Trainer`` will do the heavy lifting for us. In this tutorial, we only specify the bare minimum of parameters such as the number of epochs for model fitting `max_epochs`, and the GPU used for fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # you can fit for longer but 50 epochs should alredy yield a decent result\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=pl.loggers.CSVLogger(\"csv_logs\", name=\"tutorial\"),  # the logger creates a folder \"csv_logs\" and saves all logs as csv files there\n",
    ")\n",
    "trainer.fit(unet, fitting_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine the full FBP reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomo_ref = refine_tomogram(\n",
    "    tomo=tomo_full.cuda(1), \n",
    "    lightning_model=unet.cuda(1),\n",
    "    subtomo_size=80, \n",
    "    extraction_strides=[40, 40, 40], \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "plot_tomo_slices(tomo_ref.cpu(), figsize=(20, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
