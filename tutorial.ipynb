{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeepDeWedge Tutorial\n",
                "\n",
                "This is a minimal example for how to apply DeepDeWedge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import math\n",
                "from setup_dataset import setup_fitting_and_val_dataset\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning import seed_everything\n",
                "from utils.mrctools import load_mrc_data\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "from unet import Unet3D\n",
                "from masked_loss import masked_loss\n",
                "from normalization import get_avg_model_input_mean_and_var\n",
                "from utils.visualization import plot_tomo_slices\n",
                "import shutil\n",
                "from torchsummary import summary\n",
                "from refine_tomogram import refine_tomogram\n",
                "\n",
                "from utils.dataloader import MultiEpochsDataLoader as DataLoader\n",
                "\n",
                "\n",
                "seed_everything(42)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download the tutorial dataset\n",
                "We apply DeepDeWedge to the Wiener-Filter CTF corrected FBP reconstructon of tilt series 05 of EMPIAR-10045. First, we download the tutorial dataset which contains FBP reconstructions from the even, odd and full tilt series. We binned these reconstructions by a factor of 6 using average pooling, which results in a physical voxel size of 13.02 Angstroms. For CTF correction, we used the Wiener-like filter implemented in IsoNet (https://github.com/IsoNet-cryoET/IsoNet).\n",
                "\n",
                "The following two lines of code download the data as .zip archive and unzip it. Upon successful completion, you will find a new subdirectory \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# download data with wget\n",
                "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_yuI2Xu2ISnuBKT3FS_9cqdAXvC2Shh8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1_yuI2Xu2ISnuBKT3FS_9cqdAXvC2Shh8\" -O tutorial_data.zip && rm -rf /tmp/cookies.txt\n",
                "# !unzip tutorial_data.zip\n",
                "# !rm tutorial_data.zip"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tomo_full = load_mrc_data(\"./tutorial_data/IS002_291013_006_FullFBP_Bin6_Wiener.mrc\")\n",
                "plot_tomo_slices(\n",
                "    tomo_full.clamp(-3 * tomo_full.std(), 3 * tomo_full.std()), figsize=(20, 10)\n",
                ").show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup datasets for model fitting\n",
                "We first load the dataset for model fitting. The function `setup_fitting_and_val_dataset` returns two torch datasets, one for model fitting and one for validation to prevent overfitting. The tomograms corresponding to the filepaths in `tomo0_files` are used to construct the model inputs, while the ones in `tomo1_files` are used to construct the targets. \n",
                "\n",
                "Both the fitting and the validation dataset return model inputs and targets with shape `subtomo_size x subtomo_size x subtomo_size`. These subtomograms are extracted from the tomograms using x, y and z direction strides specified in `subtomo_extraction_strides`. To reduce RAM consumption during model fitting, all subtomograms are saved to the directory `save_subtomos_to`, which is created if it doe note exist.\n",
                "\n",
                "The number of elements in the validation set is at most `validation_frac` times the number of total extracted subtomograms. It may also contain fewer subtomograms since we randomly sample the validation subtomograms such that they have no overlap with the ones used for model fitting. If the sampling procedure was unable to sample enough validation subtomograms, the function prints a warning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "shutil.rmtree(\"./subtomos/\", ignore_errors=True)\n",
                "\n",
                "fitting_dataset, val_dataset = setup_fitting_and_val_dataset(\n",
                "    tomo0_files=[\"./tutorial_data/IS002_291013_006_EvenFBP_Bin6_Wiener.mrc\"],\n",
                "    tomo1_files=[\"./tutorial_data/IS002_291013_006_OddFBP_Bin6_Wiener.mrc\"],\n",
                "    subtomo_size=80,\n",
                "    subtomo_extraction_strides=[40, 40, 40],\n",
                "    mw_angle=60,\n",
                "    val_fraction=0.2,\n",
                "    save_subtomos_to=\"./subtomos/\",\n",
                ")\n",
                "\n",
                "print(f\"Number of subtomograms for model fitting: {len(fitting_dataset)}\")\n",
                "print(f\"Number of subtomos for validation: {len(val_dataset)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both the fitting and the validation dataset return dictionries containing the following items:\n",
                "* `model_input`: A model input $\\tilde{\\mathbf{v}}_\\varphi^0$ with two missing wedges\n",
                "* `model_target`: A model target $\\tilde{\\mathbf{v}}_\\varphi^1$ with only one missing wedge\n",
                "* `rot_mw_mask`: The rotated missing wedge mask $\\mathbf{M}_\\varphi$\n",
                "* `mw_mask`: The original missing wedge mask $\\mathbf{M}$. This mask is the same for all elements in the dataset.\n",
                "\n",
                "**Note**: The rotation angles $\\varphi$ in the training set are always random, and are re-sampled every time an item is queried. For the validation dataset, we only sample random rotation angles once and every item always has its fixed rotation.\n",
                "\n",
                "\n",
                "Let's now have a look at the real and Fourier domain representation of some of the model inputs:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for k in range(3):\n",
                "    item = fitting_dataset[k]\n",
                "    model_input = item[\"model_input\"]\n",
                "    model_input -= model_input.mean()\n",
                "    plot_tomo_slices(item[\"model_input\"], domain=\"fourier\").show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We create a fitting and a validation dataloader which return batches of elements from the fitting and validation sets:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 5\n",
                "num_workers = 10\n",
                "\n",
                "fitting_dataloader = DataLoader(\n",
                "    dataset=fitting_dataset,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    pin_memory=True,\n",
                ")\n",
                "val_dataloader = DataLoader(\n",
                "    dataset=val_dataset,\n",
                "    batch_size=batch_size,\n",
                "    num_workers=num_workers,\n",
                "    shuffle=False,\n",
                "    pin_memory=True,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fit the model\n",
                "\n",
                "First, we setup a 3D U-Net. The architecture we implemented in `unet.py`is the same used in the official IsoNet implementation.\n",
                "Before we can start model fitting, we calculate the average mean and variance of the model inputs in the fitting dataset. We use these values to normalize the input to the U-Net during model fitting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "unet_params = {\n",
                "    \"in_chans\": 1,\n",
                "    \"out_chans\": 1,\n",
                "    \"chans\": 64,\n",
                "    \"num_downsample_layers\": 3,\n",
                "    \"drop_prob\": 0.0,\n",
                "}\n",
                "\n",
                "avg_model_input_mean, avg_model_input_var = get_avg_model_input_mean_and_var(\n",
                "    fitting_dataloader,\n",
                "    batches=3\n",
                "    * len(\n",
                "        fitting_dataloader\n",
                "    ),  # compute mean and variance of the model inputs using three passes through the entire dataset\n",
                "    verbose=True,\n",
                ")\n",
                "unet_params[\"normalization_loc\"] = avg_model_input_mean\n",
                "unet_params[\"normalization_scale\"] = math.sqrt(avg_model_input_var)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For conventient model fitting, we use the [PyTorch lightning](https://lightning.ai/pytorch-lightning/) framework. Below, we define the class `LitUnet3D`, which takes parameters for a U-Net and the `torch.optim.Adam` optimizer as input and can then be used to fit the U-Net using PyTroch lightning. The two most important methods of this class are:\n",
                "* `training_step`: This step handles passing the model inputs provided by the fitting dataloader through the model and calculating the loss. \n",
                "+ `validation_step`: In this step, we can implement any validation routine we like. For simplicity, we just calculate the loss on the validation set to monitor overfitting. Depending on which logger we use, we can for example also log plots of the model output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LitUnet3D(pl.LightningModule):\n",
                "    def __init__(self, unet_params, adam_params):\n",
                "        super().__init__()\n",
                "        self.unet_params = unet_params\n",
                "        self.adam_params = adam_params\n",
                "        self.unet = Unet3D(**self.unet_params)\n",
                "        self.save_hyperparameters()\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.unet(x.unsqueeze(1)).squeeze(\n",
                "            1\n",
                "        )  # unsqueeze to add channel dimension, squeeze to remove it\n",
                "\n",
                "    def training_step(self, batch, batch_idx):\n",
                "        model_output = self(batch[\"model_input\"])\n",
                "        loss = masked_loss(\n",
                "            model_output=model_output,\n",
                "            target=batch[\"model_target\"],\n",
                "            rot_mw_mask=batch[\"rot_mw_mask\"],\n",
                "            mw_mask=batch[\"mw_mask\"],\n",
                "        )\n",
                "        self.log(\n",
                "            \"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
                "        )\n",
                "        return loss\n",
                "\n",
                "    def validation_step(self, batch, batch_idx):\n",
                "        model_output = self(batch[\"model_input\"])\n",
                "        loss = masked_loss(\n",
                "            model_output=model_output,\n",
                "            target=batch[\"model_target\"],\n",
                "            rot_mw_mask=batch[\"rot_mw_mask\"],\n",
                "            mw_mask=batch[\"mw_mask\"],\n",
                "        )\n",
                "        self.log(\n",
                "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
                "        )\n",
                "\n",
                "    def configure_optimizers(self):\n",
                "        optimizer = torch.optim.Adam(self.parameters(), **self.adam_params)\n",
                "        return optimizer\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lit_unet = LitUnet3D(unet_params=unet_params, adam_params={\"lr\": 4e-4})\n",
                "summary(\n",
                "    lit_unet, input_size=(80, 80, 80), device=\"cpu\"\n",
                ")  # prints the model architecture as well as the forward and backward pass size for an input sub-tomogram with shape 80x80x80\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PyTorch Lightning's ``Trainer`` will do the heavy lifting for us. In this tutorial, we only specify the bare minimum of parameters such as the number of epochs for model fitting `max_epochs`, and the GPU used for fitting. For more advanced functionality, please refer to the [documentation](https://lightning.ai/docs/pytorch/stable/common/trainer.html) of the ``Trainer`` class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = pl.Trainer(\n",
                "    max_epochs=50,  # you can fit for longer but 50 epochs should alredy give a decent result\n",
                "    accelerator=\"gpu\",\n",
                "    devices=[3],\n",
                "    check_val_every_n_epoch=1,\n",
                "    deterministic=True,  # to ensure reproducability\n",
                "    logger=pl.loggers.CSVLogger(\n",
                "        \"csv_logs\", name=\"tutorial\"\n",
                "    ),  # the logger creates a folder \"csv_logs\" and saves all logs as csv files there\n",
                "    # logger=pl.loggers.TensorBoardLogger(\"tensorboard_logs\", name=\"tutorial\"),  # the TensorBoard logger offers more functionality than the CSVLogger\n",
                "    callbacks=[\n",
                "        pl.callbacks.ModelCheckpoint(\n",
                "            dirpath=\"./ckpts\", filename=\"{epoch}-{val_loss:.5f}\", save_top_k=-1\n",
                "        )\n",
                "    ],\n",
                ")\n",
                "trainer.fit(lit_unet, fitting_dataloader, val_dataloader)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refine the full FBP reconstruction\n",
                "\n",
                "We now apply the fitted model to the full FBP reconstruction. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tomo_ref = refine_tomogram(\n",
                "    tomo=tomo_full.cuda(1),\n",
                "    lightning_model=lit_unet.cuda(1),\n",
                "    subtomo_size=80,\n",
                "    subtomo_extraction_strides=[40, 40, 40],\n",
                "    batch_size=10,\n",
                ")\n",
                "tomo_ref = tomo_ref.cpu()\n",
                "plot_tomo_slices(\n",
                "    tomo_ref.clamp(-3 * tomo_ref.std(), 3 * tomo_ref.std()), figsize=(20, 10)\n",
                ").show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 10))\n",
                "plt.imshow(tomo_ref.clamp(-3 * tomo_ref.std(), 3 * tomo_ref.std())[:, :, 180])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.15 ('base')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
